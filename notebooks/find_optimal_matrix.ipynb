{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11b047ec-7247-4838-8995-27fcc90f0404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/chervovn04/Programming/hackathons/2022/agrocode\n"
     ]
    }
   ],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca81e5cb-1fbb-465b-9660-edd48c15957a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import string\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from PIL import Image\n",
    "from collections import Counter, OrderedDict\n",
    "%matplotlib inline\n",
    "\n",
    "from glob import glob\n",
    "from sklearn.metrics import *\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.applications.efficientnet import EfficientNetB0, EfficientNetB1, preprocess_input\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from model_structure import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5657774f-fb92-465c-8c12-5cc756c7c1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseModel(nn.Module):\n",
    "    def __init__(self, extractor):\n",
    "        super().__init__()\n",
    "        self.extractor = extractor\n",
    "    def forward(self, image_0, image_1):\n",
    "        return self.extractor(image_0), self.extractor(image_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c003518d-9d5f-4022-b137-24e112228b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d043d12c-cc8e-4976-aa70-0f8de35fedc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8650d266-d9e4-4013-806a-38cd4984c8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AP(relevance):\n",
    "    Ps = []\n",
    "    count = 0\n",
    "    for i, val in enumerate(relevance):\n",
    "        i += 1\n",
    "        if val:\n",
    "            count += 1\n",
    "            Ps.append(count / i)\n",
    "    if not Ps:\n",
    "        Ps = [0]\n",
    "    return sum(Ps) / len(Ps)\n",
    "\n",
    "def mAP(relevances):\n",
    "    return sum([AP(relevance) for relevance in relevances]) / len(relevances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bc4c979-a529-485d-a7dd-c4c226a4fa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_double_spaces(nm):\n",
    "    new_nm = ''\n",
    "    for char in nm:\n",
    "        if char != ' ' or (len(new_nm) and new_nm[-1] != ' '):\n",
    "            new_nm += char\n",
    "    return new_nm.strip()\n",
    "\n",
    "def process(nm):\n",
    "    # 1. to lower\n",
    "    nm = nm.lower()\n",
    "    \n",
    "    # 2. delete useless sequences\n",
    "    for todel in [';', '&quot']:\n",
    "        nm = nm.replace(todel, '')\n",
    "        \n",
    "    # 3. delete text in parenthesis\n",
    "    new_nm = ''\n",
    "    balance = 0\n",
    "    for char in nm:\n",
    "        if char == '(':\n",
    "            balance += 1\n",
    "        elif char == ')':\n",
    "            balance = max(0, balance - 1)\n",
    "        elif balance == 0:\n",
    "            new_nm += char\n",
    "    nm = new_nm\n",
    "    \n",
    "    # 4. only russian symbols\n",
    "    new_nm = ''\n",
    "    for char in nm:\n",
    "        if char in 'йцукенгшщзхъфывапролджэячсмитьбю ':\n",
    "            new_nm += char\n",
    "    nm = delete_double_spaces(new_nm)\n",
    "    \n",
    "    # 6. delete useless \"words\"\n",
    "    # 7. convert every word to the origin form \n",
    "    \n",
    "    new_nm = ''\n",
    "    black_list = ['х', 'хх', 'ххх', 'мм', 'с', 'км', 'от', 'в', 'т']\n",
    "    for word in nm.split(' '):\n",
    "        if word not in black_list and len(word) > 2:\n",
    "            word = morph.parse(word)[0].normal_form \n",
    "            new_nm += word + ' '\n",
    "    nm = delete_double_spaces(new_nm)\n",
    "    \n",
    "    return nm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d225805-e88b-49b8-b655-7e033cc754e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b0c4bf9-1c8e-4d76-a5db-bfbeedd4c62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(path, models_data):\n",
    "    with torch.no_grad():\n",
    "        features = []\n",
    "        for model, tfm in models_data:\n",
    "            image = Image.open(path).convert('RGB')\n",
    "            image = tfm(image)\n",
    "\n",
    "            image = image[None, :, :, :]\n",
    "                        \n",
    "            pred = np.array(model(image).detach()).squeeze()\n",
    "\n",
    "            features.append(pred)\n",
    "        features = np.concatenate(features, axis=0)\n",
    "        features /= np.linalg.norm(features)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5bfa0cb-5508-47cc-ad67-ac950b611cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_selection():\n",
    "    pass\n",
    "\n",
    "def hard_selection():\n",
    "    pass\n",
    "\n",
    "def get_results(db, que, db_path, que_path):\n",
    "    db_emb = []\n",
    "    for idx in tqdm(test.idx):\n",
    "        db_emb.append(extract_features(f'{db_path}{idx}.png', models_data))\n",
    "    db_emb = np.array(db_emb)\n",
    "\n",
    "    que_emb = []\n",
    "    for idx in tqdm(queries.idx):\n",
    "        que_emb.append(extract_features(f'{que_path}{idx}.png', models_data))\n",
    "    que_emb = np.array(que_emb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13a60745-7b64-4680-8726-afa118fc4246",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emb(data, models_data, base_dir='data/train/'):\n",
    "    emb = []\n",
    "    for idx in tqdm(data.idx):\n",
    "        emb.append(extract_features(f'{base_dir}{idx}.png', models_data))\n",
    "    emb = np.array(emb)\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf4073a4-6a55-49f0-b18e-4dd87490e790",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/train.csv')\n",
    "db, que = train_test_split(data, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa64605a-beb4-4e5f-99d6-30b6d019387b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT_H_Wrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x.pooler_output\n",
    "    \n",
    "def get_vit_transformer_model(path):\n",
    "    vit = torch.load(path)\n",
    "    vit.eval()\n",
    "    model = ViT_H_Wrapper(vit)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def get_vit_model(path):\n",
    "    model = torch.load(path)\n",
    "    model.eval()\n",
    "    model.fc = Identical()\n",
    "    return model\n",
    "\n",
    "models_data = [\n",
    "    (get_vit_model('weights/vit_b16.pt'), transforms.Compose([\n",
    "        transforms.Resize(224), \n",
    "        transforms.CenterCrop(224), \n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "    ])),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39e062fd-93c9-48a4-b0f8-307cf7e46576",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 3900/3900 [20:37<00:00,  3.15it/s]\n",
      "100%|█████████████████████████████████████████| 976/976 [05:55<00:00,  2.74it/s]\n"
     ]
    }
   ],
   "source": [
    "db_emb = get_emb(db, models_data)\n",
    "que_emb = get_emb(que, models_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9a3aee82-5265-4431-98df-20264d7ccdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_similarity_model = torch.load('weights/image_similarity.pt', map_location=torch.device('cpu'))\n",
    "image_similarity_model.eval()\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4b6045f9-7954-4679-b6fb-b177e4bd5a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def d(x, y):\n",
    "    return nn.functional.pairwise_distance(x, y)\n",
    "\n",
    "def evaluate(db, que, db_emb, que_emb, db_dir, que_dir, final_model, transform, need_processing=0):\n",
    "    with torch.no_grad():\n",
    "        first_selection = 20\n",
    "\n",
    "        neigh = NearestNeighbors(n_neighbors=first_selection, metric='cosine')\n",
    "        neigh.fit(db_emb)\n",
    "\n",
    "        distances, idxs = neigh.kneighbors(que_emb, first_selection, return_distance=True)\n",
    "\n",
    "        relevances = []\n",
    "        for i in tqdm(range(idxs.shape[0])):\n",
    "            image_0 = Image.open(f'{que_dir}{que.idx.iloc[i]}.png').convert(\"RGB\")\n",
    "            image_0 = transform(image_0)[None, :, :, :]\n",
    "\n",
    "            for j in range(idxs.shape[1]):\n",
    "                image_1 = Image.open(f'{db_dir}{db.idx.iloc[idxs[i][j]]}.png').convert(\"RGB\")\n",
    "                image_1 = transform(image_1)[None, :, :, :]\n",
    "\n",
    "                distances[i][j] = torch.sigmoid(1 - d(*final_model(image_0, image_1)))\n",
    "\n",
    "            order = np.argsort(distances[i])[::-1]\n",
    "            order = order[:10]\n",
    "\n",
    "            name = que.item_nm.iloc[i]\n",
    "            que_rec = idxs[i][order]\n",
    "\n",
    "            relevance = []\n",
    "            for idx in que_rec:\n",
    "                relevance.append(name == db.item_nm.iloc[idx])\n",
    "            relevances.append(relevance)\n",
    "        return mAP(relevances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0f179847-6bc3-406c-b3ec-9194ddaebc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 976/976 [22:28<00:00,  1.38s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.08787053493984114"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "        transforms.Resize(224), \n",
    "        transforms.CenterCrop(224), \n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "    ])\n",
    "\n",
    "evaluate(db, que, db_emb, que_emb, 'data/train/', 'data/train/', image_similarity_model, transform, image_similarity_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "548c7b00-aef6-4450-a4d8-e321bffeab4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate2(db, que, db_emb, que_emb, db_dir, que_dir, final_model, transform, need_processing=0):\n",
    "    with torch.no_grad():\n",
    "        first_selection = 10\n",
    "\n",
    "        neigh = NearestNeighbors(n_neighbors=first_selection, metric='cosine')\n",
    "        neigh.fit(db_emb)\n",
    "\n",
    "        distances, idxs = neigh.kneighbors(que_emb, first_selection, return_distance=True)\n",
    "\n",
    "        relevances = []\n",
    "        for i in tqdm(range(idxs.shape[0])):\n",
    "            name = que.item_nm.iloc[i]\n",
    "            \n",
    "            que_rec = idxs[i]\n",
    "            relevance = []\n",
    "            for idx in que_rec:\n",
    "                relevance.append(name == db.item_nm.iloc[idx])\n",
    "            relevances.append(relevance)\n",
    "        return mAP(relevances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4f170fbb-3492-4fb3-89d1-cafba51eafb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 976/976 [00:00<00:00, 9006.69it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.07869382421115081"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate2(db, que, db_emb, que_emb, 'data/train/', 'data/train/', image_similarity_model, transform, image_similarity_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956eef5f-1bdd-4aa4-bf82-97ece5d08dd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d08b80c-bdd2-42c5-bd84-b88bd312c20c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
